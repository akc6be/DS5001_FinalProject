{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "* DS 5001\n",
    "* Amber Curran\n",
    "* akc6be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was obtained from Project Gutenberg to explore the texts of the most popular and well known literature as downloaded from the site. These include those counted as the top 5 downloaded pieces of literature downloaded in the last 30 days as of today. Analysis through different text structures, terms, and sentiments are explored below. Visualizations for the code is shown at the very end and included in the supplimental report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F0: Acquire Source Texts\n",
    "\n",
    "The novels were obtained from Project Gutenberg in their list of all books sorted by popularity. Therefore, these are the top 5 most downloaded books in the last 30 days within the Project Gutenberg database:\n",
    "\n",
    "1. **Frankenstein; Or, The Modern Prometheus** by Mary Wollstonecraft Shelley --ID 11 (80658 downloads)\n",
    "2. **Pride and Prejudice** by Jane Austen --ID 1342 (56494 downloads)\n",
    "3. **A Christmas Carol in Prose; Being a Ghost Story of Christmas** by Charles Dickens --ID 25344 (44543 downloads)\n",
    "4. **The Scarlet Letter** by Nathaniel Hawthorne --ID 46 (35961 downloads)\n",
    "5. **Alice's Adventures in Wonderland** by Lewis Carroll --ID 84 (26460 downloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs\n",
    "\n",
    "Define OCHO and directory of texts used for most of the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "\n",
    "SENTS = OHCO[:4]\n",
    "PARAS = OHCO[:3]\n",
    "CHAPS = OHCO[:2]\n",
    "BOOKS = OHCO[:1]\n",
    "\n",
    "epub_dir = 'texts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import all necessary modules for most of the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2f5be3ac21b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly_express\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/plotly_express/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.4.1\"\u001b[0m  \u001b[0;31m# sync with setup.py!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly_express as px\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect\n",
    "\n",
    "Since Project Gutenberg texts vary widely in their markup, I defined the chunking patterns by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman = '[IVXLCM]+'\n",
    "caps = \"[A-Z';, -]+\"\n",
    "chap_pats = {\n",
    "    11: {\n",
    "        'start_line': 31,\n",
    "        'end_line': 3406,\n",
    "        'chapter': re.compile(\"^\\s*CHAPTER\\s+{}\\.\".format(roman))\n",
    "    },\n",
    "    1342: {\n",
    "        'start_line': 37,\n",
    "        'end_line': 14230,\n",
    "        'chapter': re.compile(\"^Chapter\\s+\\d+$\")\n",
    "    },\n",
    "    25344: {\n",
    "        'start_line': 1611,\n",
    "        'end_line': 8721,\n",
    "        'chapter': re.compile(\"^\\s*{}\\.\".format(roman))          \n",
    "    },\n",
    "    46: {\n",
    "        'start_line': 29,\n",
    "        'end_line': 3872,\n",
    "        'chapter': re.compile(\"^\\s*STAVE\\s+{}\\:\".format(roman))\n",
    "    },\n",
    "    84: {\n",
    "        'start_line': 31,\n",
    "        'end_line': 7389,\n",
    "        'chapter': re.compile(\"^Chapter\\s+\\d+$\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F0:\n",
    "\n",
    "All text files downloaded from Project Gutenberg are included in the UVA Box folder called \"texts\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1:\n",
    "\n",
    "Create a corpus of all documents. These documents are read into a dataframe including each line number and line text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of documents\n",
    "epubs = [epub for epub in sorted(glob(epub_dir+'/*.txt'))]\n",
    "\n",
    "# Create an empty dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop through documents to add text all on one dataframe with indicated line and book ID\n",
    "for epub_file in epubs:\n",
    "        \n",
    "        # Get ID from filename\n",
    "        book_id = int(epub_file.split('.')[0].replace('texts/',''))\n",
    "        \n",
    "        # Import file as lines\n",
    "        lines = open(epub_file, 'r', encoding='utf-8-sig').readlines()\n",
    "        subdf = pd.DataFrame(lines, columns=['line_str'])\n",
    "        subdf.index.name = 'line_num'\n",
    "        subdf.line_str = subdf.line_str.str.strip()\n",
    "        subdf['book_id'] = book_id\n",
    "        \n",
    "        # Set book_id as index\n",
    "        subdf = subdf.reset_index().set_index('book_id')\n",
    "        \n",
    "        # Append to empty dataframe\n",
    "        df = df.append(subdf)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F2: \n",
    "\n",
    "F2 was created by registering and chunking each book into the DOC dataframe to only include the block of text that is the text of the book without any markup. It was further parsed into each paragraph. The LIB table includes basic information on the book id, author, and title. The TOKEN table was created by listing each term and using nltk to determine the part of speech associated with the word for each word in each book. The VOCAB table includes each term and the number of times each is included in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_epubs(epub_list, chap_pats, OHCO=OHCO):\n",
    "    \n",
    "    my_lib = []\n",
    "    my_doc = []\n",
    "\n",
    "    for epub_file in epub_list:\n",
    "        \n",
    "        # Get ID from filename\n",
    "        book_id = int(epub_file.split('.')[0].replace('texts/',''))\n",
    "        print(\"BOOK ID\", book_id)\n",
    "        \n",
    "        # Import file as lines\n",
    "        lines = open(epub_file, 'r', encoding='utf-8-sig').readlines()\n",
    "        df = pd.DataFrame(lines, columns=['line_str'])\n",
    "        df.index.name = 'line_num'\n",
    "        df.line_str = df.line_str.str.strip()\n",
    "        df['book_id'] = book_id\n",
    "        \n",
    "        # FIX CHARACTERS TO IMPROVE TOKENIZATION\n",
    "        df.line_str = df.line_str.str.replace('—', ' — ')\n",
    "        df.line_str = df.line_str.str.replace('-', ' - ')\n",
    "        \n",
    "        # Get book title and put into LIB table -- note problems, though\n",
    "        book_title = re.sub(r\"The Project Gutenberg eBook( of|,) \", \"\", df.loc[0].line_str, flags=re.IGNORECASE)\n",
    "        book_title = re.sub(r\"Project Gutenberg's \", \"\", book_title, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove cruft\n",
    "        a = chap_pats[book_id]['start_line'] - 1\n",
    "        b = chap_pats[book_id]['end_line'] + 1\n",
    "        df = df.iloc[a:b]\n",
    "        \n",
    "        # Chunk by chapter\n",
    "        chap_lines = df.line_str.str.match(chap_pats[book_id]['chapter'])\n",
    "        chap_nums = [i+1 for i in range(df.loc[chap_lines].shape[0])]\n",
    "        df.loc[chap_lines, 'chap_num'] = chap_nums\n",
    "        df.chap_num = df.chap_num.ffill()\n",
    "\n",
    "        # Clean up\n",
    "        df = df[~df.chap_num.isna()] # Remove chapter heading lines\n",
    "        df = df.loc[~chap_lines] # Remove everything before Chapter 1\n",
    "        df['chap_num'] = df['chap_num'].astype('int')\n",
    "        \n",
    "        # Group -- Note that we exclude the book level in the OHCO at this point\n",
    "        df = df.groupby(OHCO[1:2]).line_str.apply(lambda x: '\\n'.join(x)).to_frame() # Make big string\n",
    "        \n",
    "        # Split into paragrpahs\n",
    "        df = df['line_str'].str.split(r'\\n\\n+', expand=True).stack().to_frame().rename(columns={0:'para_str'})\n",
    "        df.index.names = OHCO[1:3] # MAY NOT BE NECESSARY UNTIL THE END\n",
    "        df['para_str'] = df['para_str'].str.replace(r'\\n', ' ').str.strip()\n",
    "        df = df[~df['para_str'].str.match(r'^\\s*$')] # Remove empty paragraphs\n",
    "        \n",
    "        # Set index\n",
    "        df['book_id'] = book_id\n",
    "        df = df.reset_index().set_index(OHCO[:3])\n",
    "\n",
    "        # Register\n",
    "        my_lib.append((book_id, book_title, epub_file))\n",
    "        my_doc.append(df)\n",
    "\n",
    "    docs = pd.concat(my_doc)\n",
    "    library = pd.DataFrame(my_lib, columns=['book_id', 'book_title', 'book_file']).set_index('book_id')\n",
    "    return library, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints all book IDs that verify which books are added into each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epubs = [epub for epub in sorted(glob(epub_dir+'/*.txt'))]\n",
    "LIB, DOC = acquire_epubs(epubs, chap_pats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print LIB table sample\n",
    "LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the author and title into their own columns\n",
    "LIB['author'] = LIB.book_title.str.split(', by').apply(lambda x: x[1])\n",
    "LIB['title'] = LIB.book_title.str.split(', by').apply(lambda x: x[0])\n",
    "LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as LIB table\n",
    "LIB.to_csv('LIB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOC table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print DOC table sample\n",
    "DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as DOC table\n",
    "DOC.to_csv('DOC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKEN Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc_df, OHCO=OHCO, remove_pos_tuple=False, ws=False):\n",
    "    \n",
    "    # Paragraphs to Sentences\n",
    "    df = doc_df.para_str\\\n",
    "        .apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'sent_str'})\n",
    "    \n",
    "    # Sentences to Tokens\n",
    "    # Local function to pick tokenizer\n",
    "    def word_tokenize(x):\n",
    "        if ws:\n",
    "            s = pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))\n",
    "        else:\n",
    "            s = pd.Series(nltk.pos_tag(nltk.word_tokenize(x)))\n",
    "        return s\n",
    "            \n",
    "    df = df.sent_str\\\n",
    "        .apply(word_tokenize)\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'pos_tuple'})\n",
    "    \n",
    "    # Grab info from tuple\n",
    "    df['pos'] = df.pos_tuple.apply(lambda x: x[1])\n",
    "    df['token_str'] = df.pos_tuple.apply(lambda x: x[0])\n",
    "    if remove_pos_tuple:\n",
    "        df = df.drop('pos_tuple', 1)\n",
    "    \n",
    "    # Add index\n",
    "    df.index.names = OHCO\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tokenize function to create a new TOKEN table\n",
    "TOKEN = tokenize(DOC, ws=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print TOKEN table sample\n",
    "TOKEN.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as TOKEN table\n",
    "TOKEN.to_csv('TOKEN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOCAB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the TOKEN table to create a new VOCAB table \n",
    "TOKEN['term_str'] = TOKEN['token_str'].str.lower().str.replace('[\\W_]', '')\n",
    "VOCAB = TOKEN.term_str.value_counts().to_frame().rename(columns={'index':'term_str', 'term_str':'n'})\\\n",
    "    .sort_index().reset_index().rename(columns={'index':'term_str'})\n",
    "VOCAB.index.name = 'term_id'\n",
    "VOCAB['num'] = VOCAB.term_str.str.match(\"\\d+\").astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print VOCAB table sample\n",
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as VOCAB table\n",
    "VOCAB.to_csv('VOCAB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F3: \n",
    "\n",
    "F3 further annotated the VOCAB table to include more information on the terms in the corpus. Annotations were derived from nltk library as a new column was added to define if each word is a stop word ( = 1) or not ( = 0), ProterStemmmer was used to derive the stem of each word into a new column, and the maximum part of speech from the TOKEN table was derived to be added as a new column in the VOCAB table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate with Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame with all stop words\n",
    "sw = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['term_str'])\n",
    "sw = sw.reset_index().set_index('term_str')\n",
    "sw.columns = ['dummy']\n",
    "sw.dummy = 1\n",
    "\n",
    "# Create a new 'stop' column to map the list of stop words to VOCAB table\n",
    "VOCAB['stop'] = VOCAB.term_str.map(sw.dummy)\n",
    "VOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB[VOCAB.stop == 1].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate with Stem Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Use the PorterStemmer to extract stem words into a new column\n",
    "stemmer = PorterStemmer()\n",
    "VOCAB['p_stem'] = VOCAB.term_str.apply(stemmer.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate with Max Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame with maximum part of speech for each term\n",
    "table = TOKEN.groupby(['term_str', 'pos']).count().iloc[:,0].unstack().idxmax(1)\n",
    "TABLE = pd.DataFrame(table, columns=['pos_max'])\n",
    "\n",
    "# Merge with VOCAB table\n",
    "VOCAB = pd.merge(VOCAB, TABLE, on='term_str')\n",
    "VOCAB.index.name = 'term_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F4: \n",
    "\n",
    "After creating a bag of words table, vector space representations included creating a document term count matrix, term frequency table, document frequency table, inverse document frequency table, and a combined term frequency-inverse document frequency table. The bag of words table was then able to be annotated with the term frequency and term frequency -inverse document frequency data.\n",
    "\n",
    "*** need to finish ***\n",
    "\n",
    "\"F4. STADM with Vector Space models. Vector space representations\n",
    "of TOKEN data and resulting statistical data, such as term\n",
    "frequency and TFIDF.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configs\n",
    "count_method = 'n' # 'c' or 'n' # n = n tokens, c = distinct token (term) count\n",
    "tf_method = 'sum' # sum, max, log, double_norm, raw, binary\n",
    "tf_norm_k = .5 # only used for double_norm\n",
    "idf_method = 'standard' # standard, max, smooth\n",
    "gradient_cmap = 'GnBu' # YlGn, GnBu, YlGnBu; For tables; see https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html \n",
    "bag = BOOKS # Set bags to BOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add term_id to TOKEN table\n",
    "TOKEN['term_id'] = TOKEN.term_str.map(VOCAB.reset_index().set_index('term_str').term_id)\n",
    "# add term rank to VOCAB\n",
    "if 'term_rank' not in VOCAB.columns:\n",
    "    VOCAB = VOCAB.sort_values('n', ascending=False).reset_index()\n",
    "    VOCAB.index.name = 'term_rank'\n",
    "    VOCAB = VOCAB.reset_index()\n",
    "    VOCAB = VOCAB.set_index('term_id')\n",
    "    VOCAB['term_rank'] = VOCAB['term_rank'] + 1\n",
    "# add frequency * rank\n",
    "VOCAB['zipf_k'] = VOCAB.n * VOCAB.term_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create BOW\n",
    "BOW = TOKEN.groupby(bag+['term_id']).term_id.count().to_frame().rename(columns={'term_id': 'n'})\n",
    "BOW['c'] = BOW.n.astype('bool').astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document-Term Count Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create count matrix\n",
    "DTCM = BOW[count_method].unstack().fillna(0).astype('int')\n",
    "DTCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Term Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF\n",
    "print('TF method:', tf_method)\n",
    "if tf_method == 'sum':\n",
    "    TF = DTCM.T / DTCM.T.sum()\n",
    "elif tf_method == 'max':\n",
    "    TF = DTCM.T / DTCM.T.max()\n",
    "elif tf_method == 'log':\n",
    "    TF = np.log10(1 + DTCM.T)\n",
    "elif tf_method == 'raw':\n",
    "    TF = DTCM.T\n",
    "elif tf_method == 'double_norm':\n",
    "    TF = DTCM.T / DTCM.T.max()\n",
    "    TF = tf_norm_k + (1 - tf_norm_k) * TF[TF > 0]\n",
    "elif tf_method == 'binary':\n",
    "    TF = DTCM.T.astype('bool').astype('int')\n",
    "TF = TF.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Document Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DF\n",
    "DF = DTCM[DTCM > 0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Inverse Document Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF\n",
    "N = DTCM.shape[0]\n",
    "print('IDF method:', idf_method)\n",
    "if idf_method == 'standard':\n",
    "    IDF = np.log10(N / DF)\n",
    "elif idf_method == 'max':\n",
    "    IDF = np.log10(DF.max() / DF) \n",
    "elif idf_method == 'smooth':\n",
    "    IDF = np.log10((1 + N) / (1 + DF)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Term Frequency Inverse Document Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TFIDF\n",
    "TFIDF = TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFIDF Matrix using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix\n",
    "BOW['tf'] = TF.stack()\n",
    "BOW['tfidf'] = TFIDF.stack()\n",
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB['tfidf_sum'] = TFIDF.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F5: \n",
    "\n",
    "PCA, LDA, word2vec were analysize in the F5. PCA was analyzed by creating a covariance matrix, and decomposing the matrix to convert the eigen data into a workable dataframe. Principal components were then selected...\n",
    "\n",
    "*** need to finish ***\n",
    "\n",
    "LDA was analyzed...\n",
    "\n",
    "word2vec was analyzed by using the TOKEN table and converting to a corpus for Gensim to be able to analyze excluding proper nouns. A model was then created to generate word embeddings with Gensim's library. Using this model, the t-SNE graph is derived as shown in section F6 with the visualizations.\n",
    "\n",
    "The sentiment analysis was also included in the F5 where the lexicon was used to associate each TOKEN with 8 main emotions and the addition of polarity which is the difference of positive and negative sentiment. This created a new table that are visually represented in F6 with mean emotion throughout the entire book and the sentiments associated throughout the book by chapter.\n",
    "\n",
    "\n",
    "\"F5: STADM with analytical models. STADM with columns and tables added for outputs of fitting and transforming models with the data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs and Imports\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import norm\n",
    "from scipy.linalg import eigh as eig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import norm\n",
    "\n",
    "TFIDF_b = TFIDF.apply(lambda x: x / norm(x), 1)\n",
    "TFIDF_b = TFIDF_b - TFIDF_b.mean()\n",
    "COV_b = TFIDF_b.cov()\n",
    "COV_b.iloc[:5, :10].style.background_gradient()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decompose the Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time eig_vals_b, eig_vecs_b = eig(COV_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Eigen Data to Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_IDX = COV_b.index\n",
    "EIG_VEC_b = pd.DataFrame(eig_vecs_b, index=TERM_IDX, columns=TERM_IDX)\n",
    "EIG_VAL_b = pd.DataFrame(eig_vals_b, index=TERM_IDX, columns=['eig_val'])\n",
    "EIG_VAL_b.index.name = 'term_id'\n",
    "EIG_VEC_b.iloc[:5, :10].style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Principal Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine eigenvalues and eigen vectors\n",
    "EIG_PAIRS_b = EIG_VAL_b.join(EIG_VEC_b.T)\n",
    "EIG_PAIRS_b['exp_var'] = np.round((EIG_PAIRS_b.eig_val / EIG_PAIRS_b.eig_val.sum()) * 100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIG_PAIRS_b.exp_var.sort_values(ascending=False).head().plot.bar(rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick the Top 10 K components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPS_b = EIG_PAIRS_b.sort_values('exp_var', ascending=False).head(10).reset_index(drop=True)\n",
    "TOPS_b.index.name = 'comp_id'\n",
    "TOPS_b.index = [\"PC{}\".format(i) for i in TOPS_b.index.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADINGS_b = TOPS_b[TERM_IDX].T\n",
    "LOADINGS_b.index.name = 'term_id'\n",
    "LOADINGS_b.head().style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADINGS_b['term_str'] = LOADINGS_b.apply(lambda x: VOCAB.loc[int(x.name)].term_str, 1)\n",
    "lb0_pos = LOADINGS_b.sort_values('PC0', ascending=True).head(10).term_str.str.cat(sep=' ')\n",
    "lb0_neg = LOADINGS_b.sort_values('PC0', ascending=False).head(10).term_str.str.cat(sep=' ')\n",
    "lb1_pos = LOADINGS_b.sort_values('PC1', ascending=True).head(10).term_str.str.cat(sep=' ')\n",
    "lb1_neg = LOADINGS_b.sort_values('PC1', ascending=False).head(10).term_str.str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Books PC0+', lb0_pos)\n",
    "print('Books PC0-', lb0_neg)\n",
    "print('Books PC1+', lb1_pos)\n",
    "print('Books PC1-', lb1_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs and imports\n",
    "n_terms = 4000\n",
    "n_topics = 30\n",
    "max_iter = 5\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = pd.read_csv(src_corpus)\n",
    "PARAS = TOKENS[TOKENS.pos.str.match(r'^NNS?$')]\\\n",
    "    .groupby(OHCO).term_str\\\n",
    "    .apply(lambda x: ' '.join(x))\\\n",
    "    .to_frame()\\\n",
    "    .rename(columns={'term_str':'para_str'})\n",
    "\n",
    "tfv = CountVectorizer(max_features=n_terms, stop_words='english')\n",
    "tf = tfv.fit_transform(PARAS.para_str)\n",
    "TERMS = tfv.get_feature_names()\n",
    "\n",
    "lda = LDA(n_components=n_topics, max_iter=max_iter, learning_offset=50., random_state=0)\n",
    "\n",
    "THETA = pd.DataFrame(lda.fit_transform(tf), index=BOOKS.index)\n",
    "THETA.columns.name = 'topic_id'\n",
    "THETA.sample(20).style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs and Imports\n",
    "from gensim.models import word2vec\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "BAG = OHCO[:2] # Paragraphs\n",
    "window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a corpus for Gensim excluding proper nouns.\n",
    "corpus = TOKEN[~TOKEN.pos.str.match('NNPS?')]\\\n",
    "    .groupby(BAG)\\\n",
    "    .term_str.apply(lambda  x:  x.tolist())\\\n",
    "    .reset_index()['term_str'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word embeddings with Gensim's library\n",
    "model = word2vec.Word2Vec(corpus, vector_size=246, window=window, min_count=200, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "salex_csv = 'salex_nrc.csv'\n",
    "nrc_cols = \"nrc_negative nrc_positive nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy nrc_sadness nrc_surprise nrc_trust\".split()\n",
    "emo = 'polarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lexicon\n",
    "salex = pd.read_csv(salex_csv).set_index('term_str')\n",
    "salex.columns = [col.replace('nrc_','') for col in salex.columns]\n",
    "# Add polarity\n",
    "salex['polarity'] = salex.positive - salex.negative\n",
    "# Get lexicon columns\n",
    "emo_cols = \"anger anticipation disgust fear joy sadness surprise trust polarity\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = TOKEN.join(LIB, on='book_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = TOKENS.drop(columns=['book_title', 'book_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_TOKEN = TOKENS.join(salex, on='term_str', how='left')\n",
    "emo_TOKEN[emo_cols] = emo_TOKEN[emo_cols].fillna(0)\n",
    "emo_TOKEN[['term_str'] + emo_cols].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_TOKEN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for chapter sentiment\n",
    "chapOHCO = OHCO[1:2]\n",
    "def plot_sentiments(df, emo='polarity'):\n",
    "    FIG = dict(figsize=(25, 5), legend=True, fontsize=14, rot=45)\n",
    "    df[emo].plot(**FIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean emotion by book\n",
    "ALICE = emo_TOKEN.loc[11].copy()\n",
    "PRIDE = emo_TOKEN.loc[1342].copy()\n",
    "SCARLETT = emo_TOKEN.loc[25344].copy()\n",
    "CHRISTMAS = emo_TOKEN.loc[46].copy()\n",
    "FRANK = emo_TOKEN.loc[84].copy()\n",
    "\n",
    "# Sentiment by chapter\n",
    "ALICE_chaps = ALICE.groupby(chapOHCO)[emo_cols].mean()\n",
    "PRIDE_chaps = PRIDE.groupby(chapOHCO)[emo_cols].mean()\n",
    "SCARLETT_chaps = SCARLETT.groupby(chapOHCO)[emo_cols].mean()\n",
    "CHRISTMAS_chaps = CHRISTMAS.groupby(chapOHCO)[emo_cols].mean()\n",
    "FRANK_chaps = FRANK.groupby(chapOHCO)[emo_cols].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F6: Visualizations\n",
    "\n",
    "Visualizations include the TFIDF heat maps, t-SNE plot, sentiment analysis plots for each document,...\n",
    "\n",
    "*** need to finish ***\n",
    "\n",
    "\"F6: STADM converted into interactive visualization. STADM represented as a database-driven application with interactive visualization, .e.g. Jupyter notebooks and web applications.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF heatmaps\n",
    "\n",
    "The TFIDF heat maps show the frequency of each term in the VOCAB table including the annotations from F4 with the addition of analysis using Zip's K and the tfidf_sum. The values are sorted on the sum of term frequency-inverse doucment frequency value and therefore these are the top 20 terms as part of the corpus. In the first heatmap, it is noted that all of these terms are part of speech \"NNP\" or proper nouns which makes sense as the authors are often referring to the characters in the novels. Therefore, in the second table we explore the top 50 terms that are not proper nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB.sort_values('tfidf_sum', ascending=False).head(20).style.background_gradient(cmap=gradient_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of top 50 terms without proper nouns\n",
    "VOCAB.loc[VOCAB.pos_max != 'NNP', ['term_rank','term_str','pos_max','tfidf_sum']].sort_values('tfidf_sum', ascending=False)\\\n",
    "    .head(50).style.background_gradient(cmap=gradient_cmap, high=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE plot\n",
    "\n",
    "The t-distributed scholastic neighbor embedding (t-SNE) plot uses the word2vec model to create vecto space representations of each word for which can be plotted on an x-y plane. In the plot we see words that are highly associated with each other are closer in vector space on the plot than words that are not as associated with each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate coordinates to plot\n",
    "coords = pd.DataFrame(index=range(len(model.wv)))\n",
    "coords['label'] = [w for w in model.wv.key_to_index]\n",
    "coords['vector'] = coords['label'].apply(lambda x: model.wv.get_vector(x))\n",
    "# Fit to t-SNE model\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "tsne_values = tsne_model.fit_transform(coords['vector'].tolist())\n",
    "# Get x and y values\n",
    "coords['x'] = tsne_values[:,0]\n",
    "coords['y'] = tsne_values[:,1]\n",
    "# Plot\n",
    "px.scatter(coords, 'x', 'y', text='label', height=1000).update_traces(mode='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Plots\n",
    "\n",
    "The sentiment analysis plots are shown for each novel. Here I used 8 emotions with the addition of polarity to describe each novel. The bar plot represents the mean amount of emotion for the book as a whole sorted from greatest to least. For example, the greatest emotion in the novel Alice in Wonderland is joy, while the mean greatest emotion in the novel Frankenstein is fear. The line plot shows the progression of the novel per chapter and the emotions associated with each chapter. As you can see in the plots, some are more fluctuating or stagnant than others.\n",
    "\n",
    "Similarity and distance measures\n",
    "\n",
    "*** need to finish ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOOK: ALICE\")\n",
    "# Plot mean emotion by book\n",
    "ALICE[emo_cols].mean().sort_values().plot.barh()\n",
    "# Plot sentiment by chapter\n",
    "plot_sentiments(ALICE_chaps, emo_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOOK: PRIDE AND PREJUDICE\")\n",
    "# Plot mean emotion by book\n",
    "PRIDE[emo_cols].mean().sort_values().plot.barh()\n",
    "# Plot sentiment by chapter\n",
    "plot_sentiments(PRIDE_chaps, emo_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOOK: SCARLETT LETTER\")\n",
    "# Plot mean emotion by book\n",
    "SCARLETT[emo_cols].mean().sort_values().plot.barh()\n",
    "# Plot sentiment by chapter\n",
    "plot_sentiments(SCARLETT_chaps, emo_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOOK: CHRISTMAS CAROL\")\n",
    "# Plot mean emotion by book\n",
    "CHRISTMAS[emo_cols].mean().sort_values().plot.barh()\n",
    "# Plot sentiment by chapter\n",
    "plot_sentiments(CHRISTMAS_chaps, emo_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOOK: FRANKENSTEIN\")\n",
    "# Plot mean emotion by book\n",
    "FRANK[emo_cols].mean().sort_values().plot.barh()\n",
    "# Pkot sentiment by chapter\n",
    "plot_sentiments(FRANK_chaps, emo_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and Distance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLIB = LIB.drop(columns=['book_title', 'book_file'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCLIB = DOC.join(newLIB, on='book_id', how='left')\n",
    "DOCLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L0 = TFIDF.astype('bool').astype('int')\n",
    "L1 = TFIDF.apply(lambda x: x / x.sum(), 1)\n",
    "L2 = TFIDF.apply(lambda x: x / norm(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIRS = pd.DataFrame(index=pd.MultiIndex.from_product([DOC.index.tolist(), DOC.index.tolist()])).reset_index()\n",
    "#PAIRS = PAIRS[PAIRS.level_0 < PAIRS.level_1].set_index(['level_0', 'level_1'])\n",
    "#PAIRS.index.names = ['doc_a', 'doc_b']\n",
    "#PAIRS.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
