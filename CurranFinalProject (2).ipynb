{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "* DS 5001\n",
    "* Amber Curran\n",
    "* akc6be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was obtained from Project Gutenberg to explore the texts of the most popular and well known literature as downloaded from the site. These include those counted as the top 5 downloaded pieces of literature downloaded in the last 30 days as of today. Analysis through different text structures, terms, and sentiments are explored below. Visualizations for the code is shown at the very end and included in the supplimental report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F0: Acquire Source Texts\n",
    "\n",
    "The novels were obtained from Project Gutenberg in their list of all books sorted by popularity. Therefore, these are the top 5 most downloaded books in the last 30 days within the Project Gutenberg database:\n",
    "\n",
    "1. **Frankenstein; Or, The Modern Prometheus** by Mary Wollstonecraft Shelley --ID 11 (80658 downloads)\n",
    "2. **Pride and Prejudice** by Jane Austen --ID 1342 (56494 downloads)\n",
    "3. **A Christmas Carol in Prose; Being a Ghost Story of Christmas** by Charles Dickens --ID 25344 (44543 downloads)\n",
    "4. **The Scarlet Letter** by Nathaniel Hawthorne --ID 46 (35961 downloads)\n",
    "5. **Alice's Adventures in Wonderland** by Lewis Carroll --ID 84 (26460 downloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs\n",
    "\n",
    "Define OCHO and directory of texts used for most of the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "\n",
    "SENTS = OHCO[:4]\n",
    "PARAS = OHCO[:3]\n",
    "CHAPS = OHCO[:2]\n",
    "BOOKS = OHCO[:1]\n",
    "\n",
    "epub_dir = 'texts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import all necessary modules for most of the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly_express as px\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect\n",
    "\n",
    "Since Project Gutenberg texts vary widely in their markup, I defined the chunking patterns by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman = '[IVXLCM]+'\n",
    "caps = \"[A-Z';, -]+\"\n",
    "chap_pats = {\n",
    "    11: {\n",
    "        'start_line': 31,\n",
    "        'end_line': 3406,\n",
    "        'chapter': re.compile(\"^\\s*CHAPTER\\s+{}\\.\".format(roman))\n",
    "    },\n",
    "    1342: {\n",
    "        'start_line': 37,\n",
    "        'end_line': 14230,\n",
    "        'chapter': re.compile(\"^Chapter\\s+\\d+$\")\n",
    "    },\n",
    "    25344: {\n",
    "        'start_line': 1611,\n",
    "        'end_line': 8721,\n",
    "        'chapter': re.compile(\"^\\s*{}\\.\".format(roman))          \n",
    "    },\n",
    "    46: {\n",
    "        'start_line': 29,\n",
    "        'end_line': 3872,\n",
    "        'chapter': re.compile(\"^\\s*STAVE\\s+{}\\:\".format(roman))\n",
    "    },\n",
    "    84: {\n",
    "        'start_line': 31,\n",
    "        'end_line': 7389,\n",
    "        'chapter': re.compile(\"^Chapter\\s+\\d+$\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F0:\n",
    "\n",
    "All text files downloaded from Project Gutenberg are included in the UVA Box folder called \"texts\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1:\n",
    "\n",
    "Create a corpus of all documents. These documents are read into a dataframe including each line number and line text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_num</th>\n",
       "      <th>line_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>The Project Gutenberg eBook of Alice’s Adventu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>This eBook is for the use of anyone anywhere i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>most other parts of the world at no cost and w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>whatsoever. You may copy it, give it away or r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>7738</td>\n",
       "      <td>including how to make donations to the Project...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>7739</td>\n",
       "      <td>Archive Foundation, how to help produce our ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>7740</td>\n",
       "      <td>subscribe to our email newsletter to hear abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>7741</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>7742</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         line_num                                           line_str\n",
       "book_id                                                             \n",
       "11              0  The Project Gutenberg eBook of Alice’s Adventu...\n",
       "11              1                                                   \n",
       "11              2  This eBook is for the use of anyone anywhere i...\n",
       "11              3  most other parts of the world at no cost and w...\n",
       "11              4  whatsoever. You may copy it, give it away or r...\n",
       "...           ...                                                ...\n",
       "84           7738  including how to make donations to the Project...\n",
       "84           7739  Archive Foundation, how to help produce our ne...\n",
       "84           7740  subscribe to our email newsletter to hear abou...\n",
       "84           7741                                                   \n",
       "84           7742                                                   \n",
       "\n",
       "[39400 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of documents\n",
    "epubs = [epub for epub in sorted(glob(epub_dir+'/*.txt'))]\n",
    "\n",
    "# Create an empty dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop through documents to add text all on one dataframe with indicated line and book ID\n",
    "for epub_file in epubs:\n",
    "        \n",
    "        # Get ID from filename\n",
    "        book_id = int(epub_file.split('.')[0].replace('texts/',''))\n",
    "        \n",
    "        # Import file as lines\n",
    "        lines = open(epub_file, 'r', encoding='utf-8-sig').readlines()\n",
    "        subdf = pd.DataFrame(lines, columns=['line_str'])\n",
    "        subdf.index.name = 'line_num'\n",
    "        subdf.line_str = subdf.line_str.str.strip()\n",
    "        subdf['book_id'] = book_id\n",
    "        \n",
    "        # Set book_id as index\n",
    "        subdf = subdf.reset_index().set_index('book_id')\n",
    "        \n",
    "        # Append to empty dataframe\n",
    "        df = df.append(subdf)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F2: \n",
    "\n",
    "F2 was created by registering and chunking each book into the DOC dataframe to only include the block of text that is the text of the book without any markup. It was further parsed into each paragraph. The LIB table includes basic information on the book id, author, and title. The TOKEN table was created by listing each term and using nltk to determine the part of speech associated with the word for each word in each book. The VOCAB table includes each term and the number of times each is included in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_epubs(epub_list, chap_pats, OHCO=OHCO):\n",
    "    \n",
    "    my_lib = []\n",
    "    my_doc = []\n",
    "\n",
    "    for epub_file in epub_list:\n",
    "        \n",
    "        # Get ID from filename\n",
    "        book_id = int(epub_file.split('.')[0].replace('texts/',''))\n",
    "        print(\"BOOK ID\", book_id)\n",
    "        \n",
    "        # Import file as lines\n",
    "        lines = open(epub_file, 'r', encoding='utf-8-sig').readlines()\n",
    "        df = pd.DataFrame(lines, columns=['line_str'])\n",
    "        df.index.name = 'line_num'\n",
    "        df.line_str = df.line_str.str.strip()\n",
    "        df['book_id'] = book_id\n",
    "        \n",
    "        # FIX CHARACTERS TO IMPROVE TOKENIZATION\n",
    "        df.line_str = df.line_str.str.replace('—', ' — ')\n",
    "        df.line_str = df.line_str.str.replace('-', ' - ')\n",
    "        \n",
    "        # Get book title and put into LIB table -- note problems, though\n",
    "        book_title = re.sub(r\"The Project Gutenberg eBook( of|,) \", \"\", df.loc[0].line_str, flags=re.IGNORECASE)\n",
    "        book_title = re.sub(r\"Project Gutenberg's \", \"\", book_title, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove cruft\n",
    "        a = chap_pats[book_id]['start_line'] - 1\n",
    "        b = chap_pats[book_id]['end_line'] + 1\n",
    "        df = df.iloc[a:b]\n",
    "        \n",
    "        # Chunk by chapter\n",
    "        chap_lines = df.line_str.str.match(chap_pats[book_id]['chapter'])\n",
    "        chap_nums = [i+1 for i in range(df.loc[chap_lines].shape[0])]\n",
    "        df.loc[chap_lines, 'chap_num'] = chap_nums\n",
    "        df.chap_num = df.chap_num.ffill()\n",
    "\n",
    "        # Clean up\n",
    "        df = df[~df.chap_num.isna()] # Remove chapter heading lines\n",
    "        df = df.loc[~chap_lines] # Remove everything before Chapter 1\n",
    "        df['chap_num'] = df['chap_num'].astype('int')\n",
    "        \n",
    "        # Group -- Note that we exclude the book level in the OHCO at this point\n",
    "        df = df.groupby(OHCO[1:2]).line_str.apply(lambda x: '\\n'.join(x)).to_frame() # Make big string\n",
    "        \n",
    "        # Split into paragrpahs\n",
    "        df = df['line_str'].str.split(r'\\n\\n+', expand=True).stack().to_frame().rename(columns={0:'para_str'})\n",
    "        df.index.names = OHCO[1:3] # MAY NOT BE NECESSARY UNTIL THE END\n",
    "        df['para_str'] = df['para_str'].str.replace(r'\\n', ' ').str.strip()\n",
    "        df = df[~df['para_str'].str.match(r'^\\s*$')] # Remove empty paragraphs\n",
    "        \n",
    "        # Set index\n",
    "        df['book_id'] = book_id\n",
    "        df = df.reset_index().set_index(OHCO[:3])\n",
    "\n",
    "        # Register\n",
    "        my_lib.append((book_id, book_title, epub_file))\n",
    "        my_doc.append(df)\n",
    "\n",
    "    docs = pd.concat(my_doc)\n",
    "    library = pd.DataFrame(my_lib, columns=['book_id', 'book_title', 'book_file']).set_index('book_id')\n",
    "    return library, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints all book IDs that verify which books are added into each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK ID 11\n",
      "BOOK ID 1342\n",
      "BOOK ID 25344\n",
      "BOOK ID 46\n",
      "BOOK ID 84\n"
     ]
    }
   ],
   "source": [
    "epubs = [epub for epub in sorted(glob(epub_dir+'/*.txt'))]\n",
    "LIB, DOC = acquire_epubs(epubs, chap_pats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_file</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alice’s Adventures in Wonderland, by Lewis Car...</td>\n",
       "      <td>texts/11.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>Pride and Prejudice, by Jane Austen</td>\n",
       "      <td>texts/1342.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25344</th>\n",
       "      <td>The Scarlet Letter, by Nathaniel Hawthorne</td>\n",
       "      <td>texts/25344.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>A Christmas Carol, by Charles Dickens</td>\n",
       "      <td>texts/46.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Frankenstein, by Mary Wollstonecraft (Godwin) ...</td>\n",
       "      <td>texts/84.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                book_title        book_file\n",
       "book_id                                                                    \n",
       "11       Alice’s Adventures in Wonderland, by Lewis Car...     texts/11.txt\n",
       "1342                   Pride and Prejudice, by Jane Austen   texts/1342.txt\n",
       "25344           The Scarlet Letter, by Nathaniel Hawthorne  texts/25344.txt\n",
       "46                   A Christmas Carol, by Charles Dickens     texts/46.txt\n",
       "84       Frankenstein, by Mary Wollstonecraft (Godwin) ...     texts/84.txt"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print LIB table sample\n",
    "LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_file</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alice’s Adventures in Wonderland, by Lewis Car...</td>\n",
       "      <td>texts/11.txt</td>\n",
       "      <td>Lewis Carroll</td>\n",
       "      <td>Alice’s Adventures in Wonderland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>Pride and Prejudice, by Jane Austen</td>\n",
       "      <td>texts/1342.txt</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>Pride and Prejudice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25344</th>\n",
       "      <td>The Scarlet Letter, by Nathaniel Hawthorne</td>\n",
       "      <td>texts/25344.txt</td>\n",
       "      <td>Nathaniel Hawthorne</td>\n",
       "      <td>The Scarlet Letter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>A Christmas Carol, by Charles Dickens</td>\n",
       "      <td>texts/46.txt</td>\n",
       "      <td>Charles Dickens</td>\n",
       "      <td>A Christmas Carol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Frankenstein, by Mary Wollstonecraft (Godwin) ...</td>\n",
       "      <td>texts/84.txt</td>\n",
       "      <td>Mary Wollstonecraft (Godwin) Shelley</td>\n",
       "      <td>Frankenstein</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                book_title        book_file  \\\n",
       "book_id                                                                       \n",
       "11       Alice’s Adventures in Wonderland, by Lewis Car...     texts/11.txt   \n",
       "1342                   Pride and Prejudice, by Jane Austen   texts/1342.txt   \n",
       "25344           The Scarlet Letter, by Nathaniel Hawthorne  texts/25344.txt   \n",
       "46                   A Christmas Carol, by Charles Dickens     texts/46.txt   \n",
       "84       Frankenstein, by Mary Wollstonecraft (Godwin) ...     texts/84.txt   \n",
       "\n",
       "                                        author  \\\n",
       "book_id                                          \n",
       "11                               Lewis Carroll   \n",
       "1342                               Jane Austen   \n",
       "25344                      Nathaniel Hawthorne   \n",
       "46                             Charles Dickens   \n",
       "84        Mary Wollstonecraft (Godwin) Shelley   \n",
       "\n",
       "                                    title  \n",
       "book_id                                    \n",
       "11       Alice’s Adventures in Wonderland  \n",
       "1342                  Pride and Prejudice  \n",
       "25344                  The Scarlet Letter  \n",
       "46                      A Christmas Carol  \n",
       "84                           Frankenstein  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the author and title into their own columns\n",
    "LIB['author'] = LIB.book_title.str.split(', by').apply(lambda x: x[1])\n",
    "LIB['title'] = LIB.book_title.str.split(', by').apply(lambda x: x[0])\n",
    "LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as LIB table\n",
    "LIB.to_csv('LIB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOC table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>para_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">11</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">13</th>\n",
       "      <th>0</th>\n",
       "      <td>Down the Rabbit - Hole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice was beginning to get very tired of sitti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So she was considering in her own mind (as wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There was nothing so _very_ remarkable in that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In another moment down went Alice after it, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">84</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">48</th>\n",
       "      <th>78</th>\n",
       "      <td>“But it is true that I am a wretch. I have mur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>“Fear not that I shall be the instrument of fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>“Farewell! I leave you, and in you the last of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>“But soon,” he cried with sad and solemn enthu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>He sprang from the cabin - window as he said t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5115 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    para_str\n",
       "book_id chap_num para_num                                                   \n",
       "11      13       0                                    Down the Rabbit - Hole\n",
       "                 1         Alice was beginning to get very tired of sitti...\n",
       "                 2         So she was considering in her own mind (as wel...\n",
       "                 3         There was nothing so _very_ remarkable in that...\n",
       "                 4         In another moment down went Alice after it, ne...\n",
       "...                                                                      ...\n",
       "84      48       78        “But it is true that I am a wretch. I have mur...\n",
       "                 79        “Fear not that I shall be the instrument of fu...\n",
       "                 80        “Farewell! I leave you, and in you the last of...\n",
       "                 81        “But soon,” he cried with sad and solemn enthu...\n",
       "                 82        He sprang from the cabin - window as he said t...\n",
       "\n",
       "[5115 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print DOC table sample\n",
    "DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as DOC table\n",
    "DOC.to_csv('DOC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKEN Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc_df, OHCO=OHCO, remove_pos_tuple=False, ws=False):\n",
    "    \n",
    "    # Paragraphs to Sentences\n",
    "    df = doc_df.para_str\\\n",
    "        .apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'sent_str'})\n",
    "    \n",
    "    # Sentences to Tokens\n",
    "    # Local function to pick tokenizer\n",
    "    def word_tokenize(x):\n",
    "        if ws:\n",
    "            s = pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))\n",
    "        else:\n",
    "            s = pd.Series(nltk.pos_tag(nltk.word_tokenize(x)))\n",
    "        return s\n",
    "            \n",
    "    df = df.sent_str\\\n",
    "        .apply(word_tokenize)\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'pos_tuple'})\n",
    "    \n",
    "    # Grab info from tuple\n",
    "    df['pos'] = df.pos_tuple.apply(lambda x: x[1])\n",
    "    df['token_str'] = df.pos_tuple.apply(lambda x: x[0])\n",
    "    if remove_pos_tuple:\n",
    "        df = df.drop('pos_tuple', 1)\n",
    "    \n",
    "    # Add index\n",
    "    df.index.names = OHCO\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tokenize function to create a new TOKEN table\n",
    "TOKEN = tokenize(DOC, ws=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25344</th>\n",
       "      <th>15</th>\n",
       "      <th>14</th>\n",
       "      <th>0</th>\n",
       "      <th>15</th>\n",
       "      <td>(the, DT)</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <th>3</th>\n",
       "      <th>24</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <td>(time, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">25344</th>\n",
       "      <th>21</th>\n",
       "      <th>17</th>\n",
       "      <th>0</th>\n",
       "      <th>31</th>\n",
       "      <td>(to, TO)</td>\n",
       "      <td>TO</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>17</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>(“Come, VB)</td>\n",
       "      <td>VB</td>\n",
       "      <td>“Come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <th>74</th>\n",
       "      <th>3</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <td>(I, PRP)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25344</th>\n",
       "      <th>2</th>\n",
       "      <th>23</th>\n",
       "      <th>7</th>\n",
       "      <th>54</th>\n",
       "      <td>(feeding, VBG)</td>\n",
       "      <td>VBG</td>\n",
       "      <td>feeding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <th>95</th>\n",
       "      <th>28</th>\n",
       "      <th>2</th>\n",
       "      <th>6</th>\n",
       "      <td>(so, RB)</td>\n",
       "      <td>RB</td>\n",
       "      <td>so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25344</th>\n",
       "      <th>7</th>\n",
       "      <th>15</th>\n",
       "      <th>0</th>\n",
       "      <th>5</th>\n",
       "      <td>(Hester, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Hester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <th>35</th>\n",
       "      <th>17</th>\n",
       "      <th>2</th>\n",
       "      <th>12</th>\n",
       "      <td>(but, CC)</td>\n",
       "      <td>CC</td>\n",
       "      <td>but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <th>109</th>\n",
       "      <th>20</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <td>(may, MD)</td>\n",
       "      <td>MD</td>\n",
       "      <td>may</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   pos_tuple  pos token_str\n",
       "book_id chap_num para_num sent_num token_num                               \n",
       "25344   15       14       0        15              (the, DT)   DT       the\n",
       "46      3        24       0        1              (time, NN)   NN      time\n",
       "25344   21       17       0        31               (to, TO)   TO        to\n",
       "        12       17       0        0             (“Come, VB)   VB     “Come\n",
       "1342    74       3        3        4                (I, PRP)  PRP         I\n",
       "25344   2        23       7        54         (feeding, VBG)  VBG   feeding\n",
       "1342    95       28       2        6                (so, RB)   RB        so\n",
       "25344   7        15       0        5           (Hester, NNP)  NNP    Hester\n",
       "84      35       17       2        12              (but, CC)   CC       but\n",
       "1342    109      20       0        1               (may, MD)   MD       may"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print TOKEN table sample\n",
    "TOKEN.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as TOKEN table\n",
    "TOKEN.to_csv('TOKEN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOCAB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the TOKEN table to create a new VOCAB table \n",
    "TOKEN['term_str'] = TOKEN['token_str'].str.lower().str.replace('[\\W_]', '')\n",
    "VOCAB = TOKEN.term_str.value_counts().to_frame().rename(columns={'index':'term_str', 'term_str':'n'})\\\n",
    "    .sort_index().reset_index().rename(columns={'index':'term_str'})\n",
    "VOCAB.index.name = 'term_id'\n",
    "VOCAB['num'] = VOCAB.term_str.str.match(\"\\d+\").astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14066</th>\n",
       "      <td>wondered</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>adornment</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>dwelling</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>calm</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13976</th>\n",
       "      <td>willing</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8488</th>\n",
       "      <td>numa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9409</th>\n",
       "      <td>plentifully</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10850</th>\n",
       "      <td>sacrifice</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>advise</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>ears</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term_str   n  num\n",
       "term_id                      \n",
       "14066       wondered  12    0\n",
       "248        adornment   4    0\n",
       "3943        dwelling  26    0\n",
       "1713            calm  39    0\n",
       "13976        willing   8    0\n",
       "8488            numa   1    0\n",
       "9409     plentifully   1    0\n",
       "10850      sacrifice  19    0\n",
       "273           advise  12    0\n",
       "3971            ears  23    0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print VOCAB table sample\n",
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as VOCAB table\n",
    "VOCAB.to_csv('VOCAB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F3: \n",
    "\n",
    "F3 further annotated the VOCAB table to include more information on the terms in the corpus. Annotations were derived from nltk library as a new column was added to define if each word is a stop word ( = 1) or not ( = 0), ProterStemmmer was used to derive the stem of each word into a new column, and the maximum part of speech from the TOKEN table was derived to be added as a new column in the VOCAB table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate with Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame with all stop words\n",
    "sw = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['term_str'])\n",
    "sw = sw.reset_index().set_index('term_str')\n",
    "sw.columns = ['dummy']\n",
    "sw.dummy = 1\n",
    "\n",
    "# Create a new 'stop' column to map the list of stop words to VOCAB table\n",
    "VOCAB['stop'] = VOCAB.term_str.map(sw.dummy)\n",
    "VOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "      <th>num</th>\n",
       "      <th>stop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3544</th>\n",
       "      <td>discerns</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4544</th>\n",
       "      <td>exordium</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5842</th>\n",
       "      <td>hang</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>calculated</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6294</th>\n",
       "      <td>idler</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6356</th>\n",
       "      <td>immoral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10411</th>\n",
       "      <td>rendezvous</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026</th>\n",
       "      <td>darted</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>features</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14103</th>\n",
       "      <td>wormwood</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           term_str   n  num  stop\n",
       "term_id                           \n",
       "3544       discerns   1    0     0\n",
       "4544       exordium   1    0     0\n",
       "5842           hang   1    0     0\n",
       "1701     calculated   5    0     0\n",
       "6294          idler   1    0     0\n",
       "6356        immoral   1    0     0\n",
       "10411    rendezvous   1    0     0\n",
       "3026         darted   6    0     0\n",
       "4804       features  35    0     0\n",
       "14103      wormwood   1    0     0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "      <th>num</th>\n",
       "      <th>stop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7101</th>\n",
       "      <td>just</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8734</th>\n",
       "      <td>other</td>\n",
       "      <td>493</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13769</th>\n",
       "      <td>was</td>\n",
       "      <td>4433</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>d</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10897</th>\n",
       "      <td>same</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>about</td>\n",
       "      <td>394</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>against</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12589</th>\n",
       "      <td>then</td>\n",
       "      <td>527</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11632</th>\n",
       "      <td>so</td>\n",
       "      <td>1480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4859</th>\n",
       "      <td>few</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term_str     n  num  stop\n",
       "term_id                          \n",
       "7101        just   173    0     1\n",
       "8734       other   493    0     1\n",
       "13769        was  4433    0     1\n",
       "2977           d     2    0     1\n",
       "10897       same   236    0     1\n",
       "52         about   394    0     1\n",
       "317      against   119    0     1\n",
       "12589       then   527    0     1\n",
       "11632         so  1480    0     1\n",
       "4859         few   171    0     1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB[VOCAB.stop == 1].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate with Stem Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Use the PorterStemmer to extract stem words into a new column\n",
    "stemmer = PorterStemmer()\n",
    "VOCAB['p_stem'] = VOCAB.term_str.apply(stemmer.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "      <th>num</th>\n",
       "      <th>stop</th>\n",
       "      <th>p_stem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7837</th>\n",
       "      <td>meagre</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>meagr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12705</th>\n",
       "      <td>tie</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>imaginative</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>imagin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6870</th>\n",
       "      <td>intervene</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>interven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841</th>\n",
       "      <td>où</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>où</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>counting</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12371</th>\n",
       "      <td>swollen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>swollen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>conferred</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>confer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8656</th>\n",
       "      <td>onion</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>onion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>formerly</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>formerli</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term_str   n  num  stop    p_stem\n",
       "term_id                                      \n",
       "7837          meagre   3    0     0     meagr\n",
       "12705            tie   3    0     0       tie\n",
       "6330     imaginative   3    0     0    imagin\n",
       "6870       intervene   1    0     0  interven\n",
       "8841              où   1    0     0        où\n",
       "2767        counting   8    0     0     count\n",
       "12371        swollen   1    0     0   swollen\n",
       "2459       conferred   4    0     0    confer\n",
       "8656           onion   2    0     0     onion\n",
       "5157        formerly  13    0     0  formerli"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate with Max Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame with maximum part of speech for each term\n",
    "table = TOKEN.groupby(['term_str', 'pos']).count().iloc[:,0].unstack().idxmax(1)\n",
    "TABLE = pd.DataFrame(table, columns=['pos_max'])\n",
    "\n",
    "# Merge with VOCAB table\n",
    "VOCAB = pd.merge(VOCAB, TABLE, on='term_str')\n",
    "VOCAB.index.name = 'term_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "      <th>num</th>\n",
       "      <th>stop</th>\n",
       "      <th>p_stem</th>\n",
       "      <th>pos_max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10690</th>\n",
       "      <td>ridges</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ridg</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>bitterness</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bitter</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13328</th>\n",
       "      <td>unprejudiced</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unprejud</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9576</th>\n",
       "      <td>precincts</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>precinct</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>chewing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chew</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7062</th>\n",
       "      <td>jot</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jot</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14090</th>\n",
       "      <td>worked</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>work</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7234</th>\n",
       "      <td>lake</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>lake</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>heavenward</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>heavenward</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11772</th>\n",
       "      <td>specimen</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>specimen</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term_str   n  num  stop      p_stem pos_max\n",
       "term_id                                                 \n",
       "10690          ridges   2    0     0        ridg     NNS\n",
       "1323       bitterness  21    0     0      bitter      NN\n",
       "13328    unprejudiced   2    0     0    unprejud      JJ\n",
       "9576        precincts   1    0     0    precinct     NNS\n",
       "1990          chewing   1    0     0        chew     VBG\n",
       "7062              jot   1    0     0         jot      NN\n",
       "14090          worked  14    0     0        work     VBD\n",
       "7234             lake  27    0     0        lake      NN\n",
       "5978       heavenward   5    0     0  heavenward      NN\n",
       "11772        specimen   2    0     0    specimen      NN"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F4: \n",
    "\n",
    "After creating a bag of words table, vector space representations included creating a document term count matrix, term frequency table, document frequency table, inverse document frequency table, and a combined term frequency-inverse document frequency table. The bag of words table was then able to be annotated with the term frequency and term frequency -inverse document frequency data.\n",
    "\n",
    "*** need to finish ***\n",
    "\n",
    "\"F4. STADM with Vector Space models. Vector space representations\n",
    "of TOKEN data and resulting statistical data, such as term\n",
    "frequency and TFIDF.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configs\n",
    "count_method = 'n' # 'c' or 'n' # n = n tokens, c = distinct token (term) count\n",
    "tf_method = 'sum' # sum, max, log, double_norm, raw, binary\n",
    "tf_norm_k = .5 # only used for double_norm\n",
    "idf_method = 'standard' # standard, max, smooth\n",
    "gradient_cmap = 'GnBu' # YlGn, GnBu, YlGnBu; For tables; see https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html \n",
    "bag = BOOKS # Set bags to BOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "      <th>num</th>\n",
       "      <th>stop</th>\n",
       "      <th>p_stem</th>\n",
       "      <th>pos_max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8302</th>\n",
       "      <td>national</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nation</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>addresses</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>address</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>canvassed</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>canvass</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5333</th>\n",
       "      <td>furniture</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>furnitur</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5497</th>\n",
       "      <td>glaciers</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>glacier</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5502</th>\n",
       "      <td>gladness</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>glad</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>cur</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cur</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>borrowed</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>borrow</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6138</th>\n",
       "      <td>homeward</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>homeward</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8424</th>\n",
       "      <td>nobler</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nobler</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          term_str   n  num  stop    p_stem pos_max\n",
       "term_id                                            \n",
       "8302      national   2    0     0    nation      JJ\n",
       "203      addresses   7    0     0   address     NNS\n",
       "1745     canvassed   3    0     0   canvass     VBN\n",
       "5333     furniture  14    0     0  furnitur      NN\n",
       "5497      glaciers   3    0     0   glacier     NNS\n",
       "5502      gladness   4    0     0      glad      NN\n",
       "2935           cur   1    0     0       cur      NN\n",
       "1450      borrowed   1    0     0    borrow     VBN\n",
       "6138      homeward   6    0     0  homeward      NN\n",
       "8424        nobler   2    0     0    nobler      JJ"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add term_id to TOKEN table\n",
    "TOKEN['term_id'] = TOKEN.term_str.map(VOCAB.reset_index().set_index('term_str').term_id)\n",
    "# add term rank to VOCAB\n",
    "if 'term_rank' not in VOCAB.columns:\n",
    "    VOCAB = VOCAB.sort_values('n', ascending=False).reset_index()\n",
    "    VOCAB.index.name = 'term_rank'\n",
    "    VOCAB = VOCAB.reset_index()\n",
    "    VOCAB = VOCAB.set_index('term_id')\n",
    "    VOCAB['term_rank'] = VOCAB['term_rank'] + 1\n",
    "# add frequency * rank\n",
    "VOCAB['zipf_k'] = VOCAB.n * VOCAB.term_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create BOW\n",
    "BOW = TOKEN.groupby(bag+['term_id']).term_id.count().to_frame().rename(columns={'term_id': 'n'})\n",
    "BOW['c'] = BOW.n.astype('bool').astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document-Term Count Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>term_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14217</th>\n",
       "      <th>14218</th>\n",
       "      <th>14219</th>\n",
       "      <th>14220</th>\n",
       "      <th>14221</th>\n",
       "      <th>14222</th>\n",
       "      <th>14223</th>\n",
       "      <th>14224</th>\n",
       "      <th>14225</th>\n",
       "      <th>14226</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>542</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>724</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25344</th>\n",
       "      <td>1134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "term_id  0      1      2      3      4      5      6      7      8      9      \\\n",
       "book_id                                                                         \n",
       "11         542      0      0      0      0      0      0      0      0      0   \n",
       "46         399      0      0      0      0      0      0      0      0      0   \n",
       "84         260      1      2      2      1      0     10      2      1      1   \n",
       "1342       724      0      0      0      0      1      0      1      0      1   \n",
       "25344     1134      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "term_id  ...  14217  14218  14219  14220  14221  14222  14223  14224  14225  \\\n",
       "book_id  ...                                                                  \n",
       "11       ...      0      0      7      0      1      0      0      1      0   \n",
       "46       ...      0      0      0      1      0      0      0      0      0   \n",
       "84       ...      3      0      0      4      0      0      0      0      0   \n",
       "1342     ...      0      1      0      0      0      0      0      0      4   \n",
       "25344    ...      5      0      0      4      0      1      5      1      0   \n",
       "\n",
       "term_id  14226  \n",
       "book_id         \n",
       "11           0  \n",
       "46           0  \n",
       "84           0  \n",
       "1342         1  \n",
       "25344        0  \n",
       "\n",
       "[5 rows x 14227 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create count matrix\n",
    "DTCM = BOW[count_method].unstack().fillna(0).astype('int')\n",
    "DTCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Term Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF method: sum\n"
     ]
    }
   ],
   "source": [
    "# Compute TF\n",
    "print('TF method:', tf_method)\n",
    "if tf_method == 'sum':\n",
    "    TF = DTCM.T / DTCM.T.sum()\n",
    "elif tf_method == 'max':\n",
    "    TF = DTCM.T / DTCM.T.max()\n",
    "elif tf_method == 'log':\n",
    "    TF = np.log10(1 + DTCM.T)\n",
    "elif tf_method == 'raw':\n",
    "    TF = DTCM.T\n",
    "elif tf_method == 'double_norm':\n",
    "    TF = DTCM.T / DTCM.T.max()\n",
    "    TF = tf_norm_k + (1 - tf_norm_k) * TF[TF > 0]\n",
    "elif tf_method == 'binary':\n",
    "    TF = DTCM.T.astype('bool').astype('int')\n",
    "TF = TF.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Document Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DF\n",
    "DF = DTCM[DTCM > 0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Inverse Document Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF method: standard\n"
     ]
    }
   ],
   "source": [
    "# Compute IDF\n",
    "N = DTCM.shape[0]\n",
    "print('IDF method:', idf_method)\n",
    "if idf_method == 'standard':\n",
    "    IDF = np.log10(N / DF)\n",
    "elif idf_method == 'max':\n",
    "    IDF = np.log10(DF.max() / DF) \n",
    "elif idf_method == 'smooth':\n",
    "    IDF = np.log10((1 + N) / (1 + DF)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Term Frequency Inverse Document Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TFIDF\n",
    "TFIDF = TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>term_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14217</th>\n",
       "      <th>14218</th>\n",
       "      <th>14219</th>\n",
       "      <th>14220</th>\n",
       "      <th>14221</th>\n",
       "      <th>14222</th>\n",
       "      <th>14223</th>\n",
       "      <th>14224</th>\n",
       "      <th>14225</th>\n",
       "      <th>14226</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "term_id  0         1         2         3         4         5         6      \\\n",
       "book_id                                                                      \n",
       "11         0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "46         0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "84         0.0  0.000009  0.000019  0.000019  0.000009  0.000000  0.000093   \n",
       "1342       0.0  0.000000  0.000000  0.000000  0.000000  0.000006  0.000000   \n",
       "25344      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "term_id     7         8         9      ...     14217     14218    14219  \\\n",
       "book_id                                ...                                \n",
       "11       0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00018   \n",
       "46       0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00000   \n",
       "84       0.000011  0.000009  0.000005  ...  0.000016  0.000000  0.00000   \n",
       "1342     0.000003  0.000000  0.000003  ...  0.000000  0.000006  0.00000   \n",
       "25344    0.000000  0.000000  0.000000  ...  0.000028  0.000000  0.00000   \n",
       "\n",
       "term_id     14220     14221    14222    14223     14224     14225     14226  \n",
       "book_id                                                                      \n",
       "11       0.000000  0.000026  0.00000  0.00000  0.000015  0.000000  0.000000  \n",
       "46       0.000008  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
       "84       0.000012  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
       "1342     0.000000  0.000000  0.00000  0.00000  0.000000  0.000023  0.000006  \n",
       "25344    0.000013  0.000000  0.00001  0.00005  0.000006  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 14227 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFIDF Matrix using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>c</th>\n",
       "      <th>tf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">11</th>\n",
       "      <th>0</th>\n",
       "      <td>542</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>632</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023231</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">25344</th>\n",
       "      <th>14217</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14220</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14222</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14223</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14224</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27530 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   n  c        tf     tfidf\n",
       "book_id term_id                            \n",
       "11      0        542  1  0.019923  0.000000\n",
       "        21       632  1  0.023231  0.000000\n",
       "        36         1  1  0.000037  0.000008\n",
       "        41         1  1  0.000037  0.000000\n",
       "        52        94  1  0.003455  0.000000\n",
       "...              ... ..       ...       ...\n",
       "25344   14217      5  1  0.000071  0.000028\n",
       "        14220      4  1  0.000057  0.000013\n",
       "        14222      1  1  0.000014  0.000010\n",
       "        14223      5  1  0.000071  0.000050\n",
       "        14224      1  1  0.000014  0.000006\n",
       "\n",
       "[27530 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix\n",
    "BOW['tf'] = TF.stack()\n",
    "BOW['tfidf'] = TFIDF.stack()\n",
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB['tfidf_sum'] = TFIDF.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F5: \n",
    "\n",
    "PCA, LDA, word2vec were analysize in the F5. PCA was analyzed by creating a covariance matrix, and decomposing the matrix to convert the eigen data into a workable dataframe. Principal components were then selected...\n",
    "\n",
    "*** need to finish ***\n",
    "\n",
    "LDA was analyzed...\n",
    "\n",
    "word2vec was analyzed by using the TOKEN table and converting to a corpus for Gensim to be able to analyze excluding proper nouns. A model was then created to generate word embeddings with Gensim's library. Using this model, the t-SNE graph is derived as shown in section F6 with the visualizations.\n",
    "\n",
    "The sentiment analysis was also included in the F5 where the lexicon was used to associate each TOKEN with 8 main emotions and the addition of polarity which is the difference of positive and negative sentiment. This created a new table that are visually represented in F6 with mean emotion throughout the entire book and the sentiments associated throughout the book by chapter.\n",
    "\n",
    "\n",
    "\"F5: STADM with analytical models. STADM with columns and tables added for outputs of fitting and transforming models with the data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs and Imports\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import norm\n",
    "from scipy.linalg import eigh as eig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col0,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col1,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col2,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col3,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col4,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col6,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col7,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col8,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col9,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col0,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col0,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col5,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col0,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col5,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col0{\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col5,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col1,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col2,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col3,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col4,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col6,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col7,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col8,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col9,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col1,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col2,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col3,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col4,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col6,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col7,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col8,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col9{\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col1,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col2,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col3,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col4,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col5,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col6,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col7,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col8,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col9,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col1,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col2,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col3,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col4,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col5,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col6,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col7,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col8,#T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col9{\n",
       "            background-color:  #73a9cf;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425\" ><thead>    <tr>        <th class=\"index_name level0\" >term_id</th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>        <th class=\"col_heading level0 col8\" >8</th>        <th class=\"col_heading level0 col9\" >9</th>    </tr>    <tr>        <th class=\"index_name level0\" >term_id</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col2\" class=\"data row0 col2\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col8\" class=\"data row0 col8\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row0_col9\" class=\"data row0 col9\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col1\" class=\"data row1 col1\" >0.000004</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col2\" class=\"data row1 col2\" >0.000009</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col3\" class=\"data row1 col3\" >0.000009</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col4\" class=\"data row1 col4\" >0.000004</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col5\" class=\"data row1 col5\" >-0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col6\" class=\"data row1 col6\" >0.000045</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col7\" class=\"data row1 col7\" >0.000005</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col8\" class=\"data row1 col8\" >0.000004</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row1_col9\" class=\"data row1 col9\" >0.000002</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col1\" class=\"data row2 col1\" >0.000009</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col2\" class=\"data row2 col2\" >0.000018</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col3\" class=\"data row2 col3\" >0.000018</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col4\" class=\"data row2 col4\" >0.000009</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col5\" class=\"data row2 col5\" >-0.000001</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col6\" class=\"data row2 col6\" >0.000089</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col7\" class=\"data row2 col7\" >0.000010</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col8\" class=\"data row2 col8\" >0.000009</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row2_col9\" class=\"data row2 col9\" >0.000005</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col0\" class=\"data row3 col0\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col1\" class=\"data row3 col1\" >0.000009</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col2\" class=\"data row3 col2\" >0.000018</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col3\" class=\"data row3 col3\" >0.000018</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col4\" class=\"data row3 col4\" >0.000009</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col5\" class=\"data row3 col5\" >-0.000001</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col6\" class=\"data row3 col6\" >0.000089</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col7\" class=\"data row3 col7\" >0.000010</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col8\" class=\"data row3 col8\" >0.000009</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row3_col9\" class=\"data row3 col9\" >0.000005</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col0\" class=\"data row4 col0\" >0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col1\" class=\"data row4 col1\" >0.000004</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col2\" class=\"data row4 col2\" >0.000009</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col3\" class=\"data row4 col3\" >0.000009</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col4\" class=\"data row4 col4\" >0.000004</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col5\" class=\"data row4 col5\" >-0.000000</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col6\" class=\"data row4 col6\" >0.000045</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col7\" class=\"data row4 col7\" >0.000005</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col8\" class=\"data row4 col8\" >0.000004</td>\n",
       "                        <td id=\"T_ce0b7c9e_5c70_11ec_9bb5_c3a87e4de425row4_col9\" class=\"data row4 col9\" >0.000002</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6d52061d90>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import norm\n",
    "\n",
    "TFIDF_b = TFIDF.apply(lambda x: x / norm(x), 1)\n",
    "TFIDF_b = TFIDF_b - TFIDF_b.mean()\n",
    "COV_b = TFIDF_b.cov()\n",
    "COV_b.iloc[:5, :10].style.background_gradient()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decompose the Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 56s, sys: 2.13 s, total: 11min 59s\n",
      "Wall time: 12min 6s\n"
     ]
    }
   ],
   "source": [
    "%time eig_vals_b, eig_vecs_b = eig(COV_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Eigen Data to Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col0{\n",
       "            background-color:  #73a9cf;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col1{\n",
       "            background-color:  #9ebad9;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col2,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col2,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col0,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col4,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col5,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col6,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col8,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col9,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col1,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col3,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col7{\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col3,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col3{\n",
       "            background-color:  #f8f1f8;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col4,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col6,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col8,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col1,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col9,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col0,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col3,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col7,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col2,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col5{\n",
       "            background-color:  #023858;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col5{\n",
       "            background-color:  #1e80b8;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col7{\n",
       "            background-color:  #dfddec;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col8,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col4{\n",
       "            background-color:  #02395a;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col9,#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col8{\n",
       "            background-color:  #03456c;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col0{\n",
       "            background-color:  #71a8ce;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col1{\n",
       "            background-color:  #9fbad9;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col5{\n",
       "            background-color:  #2a88bc;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col6{\n",
       "            background-color:  #023a5b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col7{\n",
       "            background-color:  #e0dded;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col9{\n",
       "            background-color:  #03446a;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col2{\n",
       "            background-color:  #d5d5e8;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col3{\n",
       "            background-color:  #c8cde4;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col4{\n",
       "            background-color:  #034871;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col5{\n",
       "            background-color:  #3991c1;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col6{\n",
       "            background-color:  #569dc8;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col7{\n",
       "            background-color:  #d2d2e7;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col8{\n",
       "            background-color:  #1c7fb8;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col1{\n",
       "            background-color:  #2d8abd;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col2{\n",
       "            background-color:  #034f7d;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col0{\n",
       "            background-color:  #f0eaf4;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col4{\n",
       "            background-color:  #034b76;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col6{\n",
       "            background-color:  #cccfe5;\n",
       "            color:  #000000;\n",
       "        }#T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col9{\n",
       "            background-color:  #056dac;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425\" ><thead>    <tr>        <th class=\"index_name level0\" >term_id</th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>        <th class=\"col_heading level0 col8\" >8</th>        <th class=\"col_heading level0 col9\" >9</th>    </tr>    <tr>        <th class=\"index_name level0\" >term_id</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col2\" class=\"data row0 col2\" >0.000000</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col8\" class=\"data row0 col8\" >0.000000</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row0_col9\" class=\"data row0 col9\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col0\" class=\"data row1 col0\" >0.001875</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col1\" class=\"data row1 col1\" >-0.002588</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col2\" class=\"data row1 col2\" >-0.001333</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col4\" class=\"data row1 col4\" >-0.000094</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col5\" class=\"data row1 col5\" >-0.000661</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col6\" class=\"data row1 col6\" >-0.000069</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col7\" class=\"data row1 col7\" >-0.000082</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col8\" class=\"data row1 col8\" >0.000057</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row1_col9\" class=\"data row1 col9\" >0.000174</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col0\" class=\"data row2 col0\" >-0.495050</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col1\" class=\"data row2 col1\" >0.636016</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col2\" class=\"data row2 col2\" >0.092565</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col3\" class=\"data row2 col3\" >0.005237</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col4\" class=\"data row2 col4\" >-0.001273</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col5\" class=\"data row2 col5\" >-0.001419</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col6\" class=\"data row2 col6\" >-0.003589</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col7\" class=\"data row2 col7\" >0.001984</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col8\" class=\"data row2 col8\" >-0.003620</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row2_col9\" class=\"data row2 col9\" >0.001941</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col0\" class=\"data row3 col0\" >0.487601</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col1\" class=\"data row3 col1\" >0.265698</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col2\" class=\"data row3 col2\" >0.371427</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col3\" class=\"data row3 col3\" >0.021705</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col4\" class=\"data row3 col4\" >-0.020964</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col5\" class=\"data row3 col5\" >-0.014679</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col6\" class=\"data row3 col6\" >-0.008193</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col7\" class=\"data row3 col7\" >0.028147</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col8\" class=\"data row3 col8\" >-0.011806</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row3_col9\" class=\"data row3 col9\" >-0.036484</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col0\" class=\"data row4 col0\" >-0.391465</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col1\" class=\"data row4 col1\" >-0.416299</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col2\" class=\"data row4 col2\" >0.407189</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col3\" class=\"data row4 col3\" >-0.000997</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col4\" class=\"data row4 col4\" >-0.001480</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col5\" class=\"data row4 col5\" >0.006687</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col6\" class=\"data row4 col6\" >-0.006017</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col7\" class=\"data row4 col7\" >-0.006386</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col8\" class=\"data row4 col8\" >-0.000506</td>\n",
       "                        <td id=\"T_7f55e65a_5c72_11ec_9bb5_c3a87e4de425row4_col9\" class=\"data row4 col9\" >-0.007157</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6d5ec6ad60>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TERM_IDX = COV_b.index\n",
    "EIG_VEC_b = pd.DataFrame(eig_vecs_b, index=TERM_IDX, columns=TERM_IDX)\n",
    "EIG_VAL_b = pd.DataFrame(eig_vals_b, index=TERM_IDX, columns=['eig_val'])\n",
    "EIG_VAL_b.index.name = 'term_id'\n",
    "EIG_VEC_b.iloc[:5, :10].style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Principal Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine eigenvalues and eigen vectors\n",
    "EIG_PAIRS_b = EIG_VAL_b.join(EIG_VEC_b.T)\n",
    "EIG_PAIRS_b['exp_var'] = np.round((EIG_PAIRS_b.eig_val / EIG_PAIRS_b.eig_val.sum()) * 100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='term_id'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEiCAYAAADptCm5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVeUlEQVR4nO3de5hcdX3H8ffuBiKSQHBdUC6VSsnXFrEP8IhYi8VWC16ioqhQAQVFBUp6QUXjpcjjXURUQqEqchHTqvAotliEIgoq1VbSCuiXu0RE2S4gxAdSSLZ/nLPMmCbZy8zumfzm/XqefXb3zJmZ7/nu7GfO/M5tYHx8HEnS5m+w6QIkSd1hoEtSIQx0SSqEgS5JhTDQJakQ8xp87vnAM4G7gbUN1iFJm5Mh4MnAD4E17Tc0GejPBK5u8PklaXO2P3BN+4QmA/1ugPvu+w3r1jW3L/zw8ALGxlY39vy9xF602IsWe9HSC70YHBxgu+22hjpD2zUZ6GsB1q0bbzTQJ2pQxV602IsWe9HSQ734f0PVbhSVpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQTe6H3rGF22zF4+Z3vggjIws7uv/Dax7lwQce6rgOSerEZh3oj5s/jyUnfq3pMvj6x1/Gg00XIanvOeQiSYUw0CWpEAa6JBXCQJekQhjoklSISfdyiYhh4AJgN6qrY9wCvDkzRyPiDuDh+gvgpMy8bJZqlSRtwlR2WxwHPpqZVwFExMeADwNvqG8/JDOvn53yNFXuky9p0gTIzHuBq9omXQscO1sFaWbcJ7/FNzf1q2m96iNikCrML2mbfGFEDFBd225ZZt7fvfKk6fPNTf1quqsxnwZWA2fUv++fmasiYj5wej398Ok84PDwgmmW0Js6XZsrib1oKaUXpSxHN/RyL6Yc6BFxKrA7sCQz1wFk5qr6+5qIOJPfXnOfkrGx1TO+Rl8vNXZ0tNl1MXvRYi+6a2RkYRHL0Q290IvBwYGNrghPKdAj4gPAPsCLM3NNPW1rYF5m/roecjkUWNmViiVJ0zaV3Rb3AJYBNwHfiwiA24ETgYsiYggYAm4Ejpu9UiVJmzKVvVxuAAY2cvNe3S1HkjRTHikqSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmF6Pyk0ZJ6lueG7y8GulQwzw3fXxxykaRCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgox6TVFI2IYuADYDVgD3AK8OTNHI2IxcB4wDIwBR2bmzbNYryRpI6ayhj4OfDQzIzOfAdwKfLi+7SxgeWYuBpYDZ89OmZKkyUwa6Jl5b2Ze1TbpWuApEbE9sDewop6+Atg7Ika6XqUkaVLTGkOPiEHgWOASYBfgrsxcC1B//0U9XZI0xyYdQ1/Pp4HVwBnAXt0oYHh4QTcepnEjIwubLqFn2IsWe9FSSi96eTmmHOgRcSqwO7AkM9dFxCpgp4gYysy1ETEE7Aismk4BY2OrWbdufFpFT+ilxo6OPtjo89uLFnvRYi+6a2RkYePLMTg4sNEV4SkNuUTEB4B9gJdn5hqAzLwHWAkcVs92GHBdZo52WrAkafqmstviHsAy4CbgexEBcHtmHgy8BTgvIt4L3AccOYu1SpI2YdJAz8wbgIGN3PZT4FndLkqSNH0eKSpJhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiHmTTZDRJwKvBLYFdgzM6+vp98BPFx/AZyUmZfNTpmSpMlMGujAV4FPAldv4LZDJgJektSsSQM9M68BiIjZr0aSNGNTWUPflAsjYgC4BliWmfdP9wGGhxd0WEJvGBlZ2HQJPcNetNiLllJ60cvL0Umg75+ZqyJiPnA6cAZw+HQfZGxsNevWjc+ogF5q7Ojog40+v71osRct9qK7RkYWNr4cg4MDG10RnvFeLpm5qv6+BjgTeM5MH0uS1LkZBXpEbB0R29Y/DwCHAiu7WJckaZqmstvip4BXAE8CroiIMWAJcFFEDAFDwI3AcbNZqCRp06ayl8tSYOkGbtqr++VIkmbKI0UlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIeZNNkNEnAq8EtgV2DMzr6+nLwbOA4aBMeDIzLx59kqVJG3KVNbQvwo8F/jZetPPApZn5mJgOXB2d0uTJE3HpIGemddk5qr2aRGxPbA3sKKetALYOyJGul+iJGkqZjqGvgtwV2auBai//6KeLklqwKRj6LNteHhB0yV0xcjIwqZL6Bn2osVetJTSi15ejpkG+ipgp4gYysy1ETEE7FhPn5axsdWsWzc+oyJ6qbGjow82+vz2osVetNiL7hoZWdj4cgwODmx0RXhGQy6ZeQ+wEjisnnQYcF1mjs7k8SRJnZs00CPiUxHxc2Bn4IqIuKG+6S3ACRFxE3BC/bskqSGTDrlk5lJg6Qam/xR41mwUJUmaPo8UlaRCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYWY1+kDRMQdwMP1F8BJmXlZp48rSZqejgO9dkhmXt+lx5IkzYBDLpJUiG6toV8YEQPANcCyzLy/S48rSZqibgT6/pm5KiLmA6cDZwCHT/XOw8MLulBC80ZGFjZdQs+wFy32oqWUXvTycnQc6Jm5qv6+JiLOBC6Zzv3Hxlazbt34jJ67lxo7Ovpgo89vL1rsRYu96K6RkYWNL8fg4MBGV4Q7GkOPiK0jYtv65wHgUGBlJ48pSZqZTtfQdwAuioghYAi4ETiu46okSdPWUaBn5m3AXl2qRZLUAXdblKRCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEPM6fYCIWAycBwwDY8CRmXlzp48rSZqebqyhnwUsz8zFwHLg7C48piRpmjpaQ4+I7YG9gRfUk1YAZ0TESGaOTnL3IYDBwYFOSmD77bbq6P7d0ulydIO9aLEXLfaiu5pejrbnH1r/toHx8fEZP3BE7AOcn5l7tE27ETg8M380yd3/GLh6xk8uSf1tf+Ca9gkdj6F34IdUBd0NrG2wDknanAwBT6bK0N/SaaCvAnaKiKHMXBsRQ8CO9fTJrGG9dxdJ0pTcuqGJHW0Uzcx7gJXAYfWkw4DrpjB+Lknqso7G0AEi4mlUuy1uB9xHtdtidqE2SdI0dBzokqTe4JGiklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqUMR0RMnqjHQJXWsVwJtrkXEQRFxUGb2xP7fBvpGRMSfRMQxTdfRCyJij4h4atN19IKIWBwRWzRdRy+IiB0iYhggM8f7LdQj4vnAl4B3RcQTm64HDPQNioglVOd2v2u96X3Xr4h4IfBPQN+HWET8PvBT4KSI6I1z0jYkIg4CLgHOiohLob9CPSIOBD4M/B1wL9XJshrPiL4LqMlExM7AO4E3ZealEbFVRCwEyMx1zVY3t+p/2lOApZmZETFvvdv77fWzBXA7cDjw7n4N9fp18X7gPVT/K+MR8QToj1CPiGdTXcjnhMz8BHA/8DFoPiP67R9yKlYDqzLzexGxG9VayPkRcW1EPAPKD7KIGKg/Ql4AXJmZV9ZvdB+JiI9ExGMv3tL/eddzA3AucHD9dXxE7BcRB/RLH+ohlr8G3pGZ3wS2BJ4BLIuICyNiQR+E+t3AKzLz+/XvHwfWtOVDY8tedDBN1cQfoP6+LbBbRBwAHA98jfosksA50Py78GyKiIHMHM/M/wHeCrw4IpZSjRU+SLWGumdEnA3VGllz1c6u9n/MiVNEA88DxoHnAkuB7wJbltwHaPUiM8eA12fmFXW4n0n1Jnc2sA1wZT1fcf2IiPkRsWVm3pGZP6pPFw7VqWy3Bl4HzS67gV5ZVK91D2Tmz6jC6xxgJDPPyMyHM/NY4J6I2LHRSmffoojYIiIGM/M8qo+SHwS+lJknZ+ZZwBlAyWtgExZFxGD9JjdxEZargK2oln8AuBPYLyLmN1TjXFkUEfPq18Uv64BfDZycme+pLwx/BHB3RGzdbKndVw8zrQBWRMSxABPXgMjM3wAnAX8SEXs1WWffB3q9ceMrwOepNvDMy8yPAl8EXhsR+9bzHQrsADzUWLGzrK0Xn6XqxRZ1qO8LfKpt1l2Bbes1liKDfb3Xxd+3bT+4mWqN9DrgSODZwEuoPtkVqa0XnwPOrP9HxjNzTWZe1TbrwcACCnuzb9tm8AXgn4GDJ/Z0qkN9ELiN6g1uj40+0Bzo60Cv9+A4DTiV6t13G+DV9c3vo9roc0FEfBJ4B/C6zLyviVpn20Z68RqAzLxxYpgpIo4GjgLeV/9Dl/jRev1eLKLuBfBfwC3AGzLzW5n5S+C59cVeirORXrx6A/O9EXgL1YbC1XNZ42yKiKAK8xMz82Kqv/3jgeMi4iiohmDrXPg88IPGiqWPA73eKr8UeE9mfgO4HBgDngaQmY9k5geo1jo+CbwkM69vqt7ZtIle7N42zxYR8ax6vtdl5o2NFDvLNtGLgOrNDTg+My+rNx4PUF1OsTiT9aKeZ+J1cQzVm1xRr4v6Yj1HZea3I2IH4HTgx8A9wKkR8c62ec/LzJuaqbTSt4GemfcC7wa+3bbB6zpgp4l56rHTGzPztsz8eVO1zrZN9GLntnkeycx/B/6s1Dc2mNrrAvhV2/zjJX5KgWm/Lg4q6XURES+JiLcCZOaP68lPBD6Umcdm5gqqT6r79NJeb51eJHqzExHbA78GyMz/XO/mR6k+UhIRrweeCrx3DsubU9PoxVHA72bme+u9HIozk9dFqUE+zV7sVm8ULWYoMiJeAHyGagPvRZl5O0Bm3kC16+qEXan2/OoZPfPOMhfaNu6cA5wTEYvq6RO7Hz0C3BQRf061y+I/NlHnXJhmL47DXvi6qKzfixVN1Dlb6uX/INUBU3dS7WPfvvwT8/0F1QFmH+ul3Zj7Zg293lL9ceAEqhfl0cCrqN6JJ9wP/CXwHKpxs6LGAyfYixZ70dLvvYiIP6Tan/6IzLy6HjM/JSKumfhkWu/dsoRqh4lDem35+2INPSK2odpoc0pmXpmZV1ONgz4dql2P6lkfpTpo5JiSxgPb2YsWe9HS772od0u9HTiwDvNBqk8fNwN71vMMZOYjwBXA8+shmJ7SF4GemQ9QvaNe2bYB40dUGzkAqI8Auwx4ar1lu0j2osVetPRzL9qGmbaaWK56V8Q7gd8Ab6+njdeh/kBm3rXxR2xO0YEeEfu2Hdn5k8wcbRvvWgdsV893NNV5SgayOuS9OPaixV609Hsv6rHx5wEvBU6uNwi3OxFYWG8z6PlTGhQb6FGdTOoaYHlE7DTxh2jbuPEocEtEvAg4FvhMr/+xZspetNiLFnvx2FDS5VQHEu4KnAWPnet9EdXRnw8A+zRU4rQUG+hUB3t8B/gd4LMRsUs9fWKZHwDeQHUEaFEbdzbAXrTYi5a+70V9YNjjqYaWXgpsFRHfAf4VWJSZD1Pt9XJRc1VOXbGBnpmjVGfCewHVu+wnIuLlwGvrWR6iGh97c0kbdzbEXrTYixZ78dgQyuXA4+oNnqcBewMPZeYd9RDTd5s+AnSqig302vbA8zLzVVRH+l1MtYWerM5lvDgzf9JgfXPJXrTYixZ7AfOBBRGxDPg01RGgRMTngaFN3bHXFBPo9cad3eufJ872dh3VH2pHqjMlXkt1BsUdATLz/iZqnW32osVetNiLDcvMXwMrqQ6gW5aZX6baUHpyZj7aZG3TVUSg1wdEXAt8MSJ+r23DzQ+At1H/sTLzj6hOLlTsAVX2osVetNiLSZ0HvCgzL47qtNFrsro2wmZlYHx8895oHdV1Hf8B+Crwp1QHARydmbdExAjwLuDrmflvzVU5N+xFi71osRf9Y7MPdICIeDJwb2auiYjzqXY/euP6GzKiutpKz5x3YTbYixZ70WIv+kMRgQ6ta2HWP59PdUa8A6lOoLN1Zp7WZH1zyV602IsWe1G+IsbQ4bHDcgfrn4+k2thzK7CM+sK1/cJetNiLFntRvmICHarzL7Sdh+Jaqg07L8zMlc1V1Qx70WIvWuxF2YoZcmlXH7J7FvDBzPzvhstplL1osRct9qJMRQY6PHZmuP9tuo5eYC9a7EWLvShPsYEuSf2mqDF0SepnBrokFcJAl6RCGOiSVAgDXeqCiNg/IjZ6nc2IODci3j+XNan/9NsZ1VSgiLiD6rwkVzRVQ2ZeDURTzy+Ba+jqYxEx0HbUpLTZcw1dm7WIuIDqmphfj4i1wClU18k8DfgD4GfAX2XmVfX8V1Fddu0AqkuN7RkRNwPHA38DPAk4HTgX+AKwB9X1JQ/f1EE4EXEA8IXM3Ln+fS/gc8DuwKXUVwGSZpNrJ9qsZeYRwJ3AksxcAFwI/AvwfuAJwFuBi+rzfk84AngTsJAq8AEOorqy+37A26nOH/5aYBfg6cBhU60pIrakOvf4BXUNXwZeOaMFlKbBNXSV5nDg0sy8tP798oj4D+BFVFelATg3M2+YuENEAHwkMx8AboiI64FvZuZt9e3fAPZqu/9k9gO2AE6vT1f7lYj42w6XS5qUga7SPAV4VUQsaZu2BfCttt9XbeB+v2r7+aEN/P6kadSwI3BX22XeoPVJQJo1BrpK0B6cq4ALMvOYKc4/G+4Gdmq/oATVOP+ts/y86nMGukrwK6qr70C1IfOHEXEgcAXV2vl+wC2Z+fM5quf7wKPA0ohYDrwU2Jff/pQgdZ0bRVWCDwHvjoj7gdcAL6O6Cs8o1Rr725jD13q9N8wrgNcD99U1XTxXz6/+5elzJakQrqFLUiEcQ5emKCKWUQ3lrO/qzHzhXNcjrc8hF0kqhEMuklQIA12SCmGgS1IhDHRJKoSBLkmF+D/I+9HpQl16oQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EIG_PAIRS_b.exp_var.sort_values(ascending=False).head().plot.bar(rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick the Top 10 K components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPS_b = EIG_PAIRS_b.sort_values('exp_var', ascending=False).head(10).reset_index(drop=True)\n",
    "TOPS_b.index.name = 'comp_id'\n",
    "TOPS_b.index = [\"PC{}\".format(i) for i in TOPS_b.index.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADINGS_b = TOPS_b[TERM_IDX].T\n",
    "LOADINGS_b.index.name = 'term_id'\n",
    "LOADINGS_b.head().style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADINGS_b['term_str'] = LOADINGS_b.apply(lambda x: VOCAB.loc[int(x.name)].term_str, 1)\n",
    "lb0_pos = LOADINGS_b.sort_values('PC0', ascending=True).head(10).term_str.str.cat(sep=' ')\n",
    "lb0_neg = LOADINGS_b.sort_values('PC0', ascending=False).head(10).term_str.str.cat(sep=' ')\n",
    "lb1_pos = LOADINGS_b.sort_values('PC1', ascending=True).head(10).term_str.str.cat(sep=' ')\n",
    "lb1_neg = LOADINGS_b.sort_values('PC1', ascending=False).head(10).term_str.str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Books PC0+', lb0_pos)\n",
    "print('Books PC0-', lb0_neg)\n",
    "print('Books PC1+', lb1_pos)\n",
    "print('Books PC1-', lb1_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs and imports\n",
    "n_terms = 4000\n",
    "n_topics = 30\n",
    "max_iter = 5\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = pd.read_csv(src_corpus)\n",
    "PARAS = TOKENS[TOKENS.pos.str.match(r'^NNS?$')]\\\n",
    "    .groupby(OHCO).term_str\\\n",
    "    .apply(lambda x: ' '.join(x))\\\n",
    "    .to_frame()\\\n",
    "    .rename(columns={'term_str':'para_str'})\n",
    "\n",
    "tfv = CountVectorizer(max_features=n_terms, stop_words='english')\n",
    "tf = tfv.fit_transform(PARAS.para_str)\n",
    "TERMS = tfv.get_feature_names()\n",
    "\n",
    "lda = LDA(n_components=n_topics, max_iter=max_iter, learning_offset=50., random_state=0)\n",
    "\n",
    "THETA = pd.DataFrame(lda.fit_transform(tf), index=BOOKS.index)\n",
    "THETA.columns.name = 'topic_id'\n",
    "THETA.sample(20).style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs and Imports\n",
    "from gensim.models import word2vec\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "BAG = OHCO[:2] # Paragraphs\n",
    "window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a corpus for Gensim excluding proper nouns.\n",
    "corpus = TOKEN[~TOKEN.pos.str.match('NNPS?')]\\\n",
    "    .groupby(BAG)\\\n",
    "    .term_str.apply(lambda  x:  x.tolist())\\\n",
    "    .reset_index()['term_str'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word embeddings with Gensim's library\n",
    "model = word2vec.Word2Vec(corpus, vector_size=246, window=window, min_count=200, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "salex_csv = 'salex_nrc.csv'\n",
    "nrc_cols = \"nrc_negative nrc_positive nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy nrc_sadness nrc_surprise nrc_trust\".split()\n",
    "emo = 'polarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lexicon\n",
    "salex = pd.read_csv(salex_csv).set_index('term_str')\n",
    "salex.columns = [col.replace('nrc_','') for col in salex.columns]\n",
    "# Add polarity\n",
    "salex['polarity'] = salex.positive - salex.negative\n",
    "# Get lexicon columns\n",
    "emo_cols = \"anger anticipation disgust fear joy sadness surprise trust polarity\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = TOKEN.join(LIB, on='book_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = TOKENS.drop(columns=['book_title', 'book_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_TOKEN = TOKENS.join(salex, on='term_str', how='left')\n",
    "emo_TOKEN[emo_cols] = emo_TOKEN[emo_cols].fillna(0)\n",
    "emo_TOKEN[['term_str'] + emo_cols].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_TOKEN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for chapter sentiment\n",
    "chapOHCO = OHCO[1:2]\n",
    "def plot_sentiments(df, emo='polarity'):\n",
    "    FIG = dict(figsize=(25, 5), legend=True, fontsize=14, rot=45)\n",
    "    df[emo].plot(**FIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean emotion by book\n",
    "ALICE = emo_TOKEN.loc[11].copy()\n",
    "PRIDE = emo_TOKEN.loc[1342].copy()\n",
    "SCARLETT = emo_TOKEN.loc[25344].copy()\n",
    "CHRISTMAS = emo_TOKEN.loc[46].copy()\n",
    "FRANK = emo_TOKEN.loc[84].copy()\n",
    "\n",
    "# Sentiment by chapter\n",
    "ALICE_chaps = ALICE.groupby(chapOHCO)[emo_cols].mean()\n",
    "PRIDE_chaps = PRIDE.groupby(chapOHCO)[emo_cols].mean()\n",
    "SCARLETT_chaps = SCARLETT.groupby(chapOHCO)[emo_cols].mean()\n",
    "CHRISTMAS_chaps = CHRISTMAS.groupby(chapOHCO)[emo_cols].mean()\n",
    "FRANK_chaps = FRANK.groupby(chapOHCO)[emo_cols].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F6: Visualizations\n",
    "\n",
    "Visualizations include the TFIDF heat maps, t-SNE plot, sentiment analysis plots for each document,...\n",
    "\n",
    "*** need to finish ***\n",
    "\n",
    "\"F6: STADM converted into interactive visualization. STADM represented as a database-driven application with interactive visualization, .e.g. Jupyter notebooks and web applications.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF heatmaps\n",
    "\n",
    "The TFIDF heat maps show the frequency of each term in the VOCAB table including the annotations from F4 with the addition of analysis using Zip's K and the tfidf_sum. The values are sorted on the sum of term frequency-inverse doucment frequency value and therefore these are the top 20 terms as part of the corpus. In the first heatmap, it is noted that all of these terms are part of speech \"NNP\" or proper nouns which makes sense as the authors are often referring to the characters in the novels. Therefore, in the second table we explore the top 50 terms that are not proper nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB.sort_values('tfidf_sum', ascending=False).head(20).style.background_gradient(cmap=gradient_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of top 50 terms without proper nouns\n",
    "VOCAB.loc[VOCAB.pos_max != 'NNP', ['term_rank','term_str','pos_max','tfidf_sum']].sort_values('tfidf_sum', ascending=False)\\\n",
    "    .head(50).style.background_gradient(cmap=gradient_cmap, high=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE plot\n",
    "\n",
    "The t-distributed scholastic neighbor embedding (t-SNE) plot uses the word2vec model to create vecto space representations of each word for which can be plotted on an x-y plane. In the plot we see words that are highly associated with each other are closer in vector space on the plot than words that are not as associated with each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate coordinates to plot\n",
    "coords = pd.DataFrame(index=range(len(model.wv)))\n",
    "coords['label'] = [w for w in model.wv.key_to_index]\n",
    "coords['vector'] = coords['label'].apply(lambda x: model.wv.get_vector(x))\n",
    "# Fit to t-SNE model\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "tsne_values = tsne_model.fit_transform(coords['vector'].tolist())\n",
    "# Get x and y values\n",
    "coords['x'] = tsne_values[:,0]\n",
    "coords['y'] = tsne_values[:,1]\n",
    "# Plot\n",
    "px.scatter(coords, 'x', 'y', text='label', height=1000).update_traces(mode='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Plots\n",
    "\n",
    "The sentiment analysis plots are shown for each novel. Here I used 8 emotions with the addition of polarity to describe each novel. The bar plot represents the mean amount of emotion for the book as a whole sorted from greatest to least. For example, the greatest emotion in the novel Alice in Wonderland is joy, while the mean greatest emotion in the novel Frankenstein is fear. The line plot shows the progression of the novel per chapter and the emotions associated with each chapter. As you can see in the plots, some are more fluctuating or stagnant than others.\n",
    "\n",
    "Similarity and distance measures\n",
    "\n",
    "*** need to finish ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOOK: ALICE\")\n",
    "# Plot mean emotion by book\n",
    "ALICE[emo_cols].mean().sort_values().plot.barh()\n",
    "# Plot sentiment by chapter\n",
    "plot_sentiments(ALICE_chaps, emo_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOOK: PRIDE AND PREJUDICE\")\n",
    "# Plot mean emotion by book\n",
    "PRIDE[emo_cols].mean().sort_values().plot.barh()\n",
    "# Plot sentiment by chapter\n",
    "plot_sentiments(PRIDE_chaps, emo_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOOK: SCARLETT LETTER\")\n",
    "# Plot mean emotion by book\n",
    "SCARLETT[emo_cols].mean().sort_values().plot.barh()\n",
    "# Plot sentiment by chapter\n",
    "plot_sentiments(SCARLETT_chaps, emo_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOOK: CHRISTMAS CAROL\")\n",
    "# Plot mean emotion by book\n",
    "CHRISTMAS[emo_cols].mean().sort_values().plot.barh()\n",
    "# Plot sentiment by chapter\n",
    "plot_sentiments(CHRISTMAS_chaps, emo_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOOK: FRANKENSTEIN\")\n",
    "# Plot mean emotion by book\n",
    "FRANK[emo_cols].mean().sort_values().plot.barh()\n",
    "# Pkot sentiment by chapter\n",
    "plot_sentiments(FRANK_chaps, emo_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and Distance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLIB = LIB.drop(columns=['book_title', 'book_file'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCLIB = DOC.join(newLIB, on='book_id', how='left')\n",
    "DOCLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L0 = TFIDF.astype('bool').astype('int')\n",
    "L1 = TFIDF.apply(lambda x: x / x.sum(), 1)\n",
    "L2 = TFIDF.apply(lambda x: x / norm(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIRS = pd.DataFrame(index=pd.MultiIndex.from_product([DOC.index.tolist(), DOC.index.tolist()])).reset_index()\n",
    "#PAIRS = PAIRS[PAIRS.level_0 < PAIRS.level_1].set_index(['level_0', 'level_1'])\n",
    "#PAIRS.index.names = ['doc_a', 'doc_b']\n",
    "#PAIRS.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
